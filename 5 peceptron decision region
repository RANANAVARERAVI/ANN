#With a suitable example demonstrate the perceptron learning law with its decision regions using python. Give the output in graphical form

import numpy as np
import matplotlib.pyplot as plt

# define the perceptron model
class Perceptron:
    def __init__(self, lr=0.1, n_iter=100):
        self.lr = lr  # learning rate
        self.n_iter = n_iter  # number of iterations
        self.weights = None  # weights of the model
        self.bias = None  # bias of the model
    
    def fit(self, X, y):
        # initialize weights and bias to zeros
        self.weights = np.zeros(X.shape[1])
        self.bias = 1
        
        # iterate through the dataset
        for i in range(self.n_iter):
            # iterate through each example
            for j in range(X.shape[0]):
                # calculate the predicted output
                pred = self.predict(X[j])
                # update the weights and bias if the prediction is wrong
                if pred != y[j]:
                    self.weights += self.lr * y[j] * X[j]
                    self.bias += self.lr * y[j]
    
    def predict(self, X):
        # calculate the output of the perceptron
        output = np.dot(X, self.weights) + self.bias
        # return the predicted label
        return np.where(output >= 0, 1, -1)

# create a sample dataset
X = np.array([[2, 2], [4, 4], [4, 0], [3, 2], [8, 4], [8, 0]])
y = np.array([1, 1, -1, 1, -1, -1])

# create a perceptron model and fit it to the data
model = Perceptron()
model.fit(X, y)

# plot the data and decision boundary
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.xlim(0, 10)
plt.ylim(0, 5)
x1 = np.linspace(0, 10)
x2 = -(model.weights[0]*x1 + model.bias)/model.weights[1]
plt.plot(x1, x2, '-r')
plt.show()
